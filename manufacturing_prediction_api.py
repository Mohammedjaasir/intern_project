# -*- coding: utf-8 -*-
"""manufacturing_prediction_api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pTJRNhVjINuzKu_6Cch3FeeoTWjvFOCd

# **Introduction**

The Predictive Maintenance Flask API is a project designed to predict machine downtime using a machine learning model. The API is built using Flask and allows users to upload a dataset, train a logistic regression model, and make predictions using specific input features. This project demonstrates the application of supervised machine learning in the manufacturing domain to improve operational efficiency and minimize machine downtime.

# **The key technologies**

Flask: For building the RESTful API.

Scikit-learn: For training and evaluating the logistic regression model.

Pyngrok: To expose the Flask application to the internet for testing.

This project showcases how predictive analysis can be integrated into manufacturing operations to aid decision-making, reduce costs, and improve productivity.

# **Install Dependencies**
"""

!pip install Flask scikit-learn pandas numpy ngrok joblib pandas

"""manufacturing_prediction_api/



    ├── app.py
    ├── data/
    │   └── sample_data.csv
    ├── model.py
    ├── requirements.txt
    └── README.md

# **Dataset**
"""

import pandas as pd

# Load your data
data = pd.read_csv('/content/predictive_maintenance.csv')

# Print the actual column names
print(data.columns)

dataset = pd.read_csv('/content/predictive_maintenance.csv')
print(dataset.head(7))

"""# Predictive Analysis for Manufacturing Operations

This project involves building a RESTful API for predicting machine downtime or production defects using manufacturing data. The API provides endpoints for uploading data, training a machine learning model, and making predictions.

---

## Features

1. **Data Upload Endpoint**:
   - Allows users to upload a CSV file containing manufacturing data.

2. **Training Endpoint**:
   - Trains a supervised ML model (e.g., Decision Tree) on the uploaded data.
   - Returns performance metrics (e.g., accuracy, F1-score).

3. **Prediction Endpoint**:
   - Accepts JSON input (e.g., `{" Air Temperature": 80, "Torque [Nm]": 120}`).
   - Returns predictions in JSON format (e.g., `{ "Downtime": "Yes", "Confidence": 0.85 }`).

---

## Setup Instructions

### Prerequisites

- Python 3.7+
- Libraries: Flask, Flask-RESTful, scikit-learn, pandas

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/manufacturing-api.git
   cd manufacturing-api
   ```

2. Install dependencies:
   ```bash
   pip install flask scikit-learn flask-restful pandas
   ```

3. Launch the API:
   ```bash
   python app.py
   ```

4. The API will run locally at `http://127.0.0.1:5000`.

---

## API Endpoints

### 1. Upload Data

**URL:** `POST /upload`

**Description:** Upload a CSV file containing manufacturing data.

**Request:**
```
dataset = pd.read_csv('/content/predictive_maintenance.csv')
print(dataset.head(7))
```

**Response:**
```json
{
  "message": "File uploaded successfully",
  "data_sample": {
      Accuracy: 0.9525
      F1-score: 0.2748091603053435
      Returned F1-score: 0.2748091603053435
  }
}
```

---

### 2. Train Model

**URL:** `POST /train`

**Description:** Train a machine learning model on the uploaded dataset.

**Request:**
```bash
POST /train
```

**Response:**
```json
{
  "message": "Model trained successfully",
  "accuracy": 0.85
}
```

---

### 3. Predict

**URL:** `POST /predict`

**Description:** Predict machine downtime based on input parameters.

**Request:**
```json
{
  "Air Temperature": 80,
  "Torque [Nm]": 120
}
```

**Response:**
```json
{
  "Downtime": "Yes",
  "Confidence": 0.92
}
```

---

## Example API Testing

You can test the API using Postman or cURL.

### Using cURL

1. Upload Data:
   ```bash
   curl -X POST -F "file=@manufacturing_data.csv" http://127.0.0.1:5000/upload
   ```

2. Train Model:
   ```bash
   curl -X POST http://127.0.0.1:5000/train
   ```

3. Predict:
   ```bash
   curl -X POST -H "Content-Type: application/json" -d '{"Temperature": 80, "Run_Time": 120}' http://127.0.0.1:5000/predict
   ```

### Using Postman

- **Upload Data**:
  1. Select `POST` method.
  2. URL: `http://127.0.0.1:5000/upload`
  3. Go to the `Body` tab -> `form-data` -> Add key `file` and select a CSV file.

- **Train Model**:
  1. Select `POST` method.
  2. URL: `http://127.0.0.1:5000/train`

- **Predict**:
  1. Select `POST` method.
  2. URL: `http://127.0.0.1:5000/predict`
  3. Go to the `Body` tab -> `raw` -> Paste JSON input (e.g., `{ "Temperature": 80, "Run_Time": 120 }`).

---

## Dataset Example

| Air temp[K] | Rotational speed[rmp] | Torque [Nm]| Target |
|------------|-------------|----------|---------------|
| 1          | 80          | 100      | 0             |
| 2          | 90          | 120      | 1             |
| 3          | 85          | 110      | 0             |

---

## License

This project is open-source and available under the MIT License

# **API Development**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score

def train_model(data_path):
    """
    Trains a machine learning model to predict machine downtime.

    Args:
        data_path: Path to the CSV file containing the data.

    Returns:
        Trained model object and F1-score.
    """
    data = pd.read_csv(data_path)
    X = data[['Air temperature [K]', 'Torque [Nm]']]
    y = data['Target']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = DecisionTreeClassifier(random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(f"Accuracy: {accuracy}")
    print(f"F1-score: {f1}")

    return model, f1

trained_model, f1 = train_model('/content/predictive_maintenance.csv')
print(f"Returned F1-score: {f1}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""# Load dataset"""

data = pd.read_csv('/content/predictive_maintenance.csv')
# Drop irrelevant columns
data = data.drop(columns=['UDI', 'Product ID', 'Failure Type'])

"""# Features and target"""

X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
y = data['Target']

"""# Normalize features"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# Train-test split"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""# Train the model"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Train the model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

"""# Evaluate the model"""

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

"""# Save model and scaler"""

import joblib

joblib.dump(model, 'model.pkl')
joblib.dump(scaler, 'scaler.pkl')

"""# ngrok"""

!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz
!tar -xvzf ngrok-v3-stable-linux-amd64.tgz

"""# Authtoken"""

!./ngrok config add-authtoken 2s1zyv3aw8XyaMdDz8OB59JwJAP_28WppWB49DzAZpLHf62Xh

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/')
def home():
    return "Flask is running on Google Colab!"

@app.route('/predict', methods=['POST'])
def predict():
    input_data = request.json
    prediction = "Positive" if input_data['value'] > 0 else "Negative"
    return jsonify({"Prediction": prediction})

if __name__ == '_main_':
    app.run(port=5000)

"""# app.run(port=5000)"""

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/')
def home():
    return "Flask is running on Colab!"

if __name__ == '_main_':
    app.run(port=5000)

!./ngrok http 5000

from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# Load the model and scaler
model = joblib.load('model.pkl')
scaler = joblib.load('scaler.pkl')

@app.route('/upload', methods=['POST'])
def upload_data():
    global data
    file = request.files['file']
    data = pd.read_csv(file)
    return "Dataset uploaded successfully!", 200

@app.route('/train', methods=['POST'])
def train_model():
    global model
    X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
    y = data['Target']
    X_scaled = scaler.fit_transform(X)
    model.fit(X_scaled, y)
    return jsonify({"message": "Model retrained successfully!"}), 200

@app.route('/predict', methods=['POST'])
def predict():
    input_data = request.json
    features = np.array([[
        input_data['Air temperature [K]'],
        input_data['Process temperature [K]'],
        input_data['Rotational speed [rpm]'],
        input_data['Torque [Nm]'],
        input_data['Tool wear [min]']
    ]])
    scaled_features = scaler.transform(features)
    prediction = model.predict(scaled_features)[0]
    return jsonify({"Failure": "Yes" if prediction == 1 else "No"})

if __name__ == '_main_':
    app.run(port=5000)

"""# Predictive Maintenance Flask API

## Overview
This project is a Flask-based API for predicting machine downtime using logistic regression. It includes endpoints for training a model and making predictions based on input features.

## Endpoints
*/train* (POST): Train the model using a CSV dataset.
*/predict* (POST): Make predictions with input features.

### Example Input for /predict:
```json
{
  "Air temperature [K]": 300,
  "Rotational speed [rpm]": 1500
}
"""

{
  "Downtime": "Yes",
  "Confidence": 0.85
}

{
  "Air temperature [K]": 298.1,
  "Process temperature [K]": 308.6,
  "Rotational speed [rpm]": 1551,
  "Torque [Nm]": 42.8,
  "Tool wear [min]": 0
}

"""# **Run the Flask Application**"""

from flask import Flask, request, jsonify
import joblib
import numpy as np
from flask_ngrok import run_with_ngrok

app = Flask(__name__)
run_with_ngrok(app)  # Enables ngrok for Colab

# Load the model and scaler
# Make sure to upload 'model.pkl' and 'scaler.pkl' to Colab
model = joblib.load('model.pkl')
scaler = joblib.load('scaler.pkl')

@app.route('/upload', methods=['POST'])
def upload_data():
    global data
    file = request.files['file']
    data = pd.read_csv(file)
    return "Dataset uploaded successfully!", 200

@app.route('/train', methods=['POST'])
def train_model():
    global model
    X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]
    y = data['Target']
    X_scaled = scaler.fit_transform(X)
    model.fit(X_scaled, y)
    return jsonify({"message": "Model retrained successfully!"}), 200

@app.route('/predict', methods=['POST'])
def predict():
    input_data = request.json
    features = np.array([[
        input_data['Air temperature [K]'],
        input_data['Process temperature [K]'],
        input_data['Rotational speed [rpm]'],
        input_data['Torque [Nm]'],
        input_data['Tool wear [min]']
    ]])
    scaled_features = scaler.transform(features)
    prediction = model.predict(scaled_features)[0]
    return jsonify({"Failure": "Yes" if prediction == 1 else "No"})

if __name__ == '__main__':
    app.run()  # Use ngrok to expose this

